{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FashionMNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "NrXBHeOT5tKA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Imports:"
      ]
    },
    {
      "metadata": {
        "id": "XVevzz7Z5tKD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import random\n",
        "import struct\n",
        "import torch\n",
        "import errno\n",
        "import math\n",
        "import gzip\n",
        "import io\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NS3d_4i_5tKQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dataset Class:"
      ]
    },
    {
      "metadata": {
        "id": "q7t2Hj4n5tKS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Fashion(Dataset):\n",
        "\t\"\"\"Dataset: https://github.com/zalandoresearch/fashion-mnist\n",
        "    Args:\n",
        "        root (string): Root directory of dataset where ``processed/training.pt``\n",
        "            and  ``processed/test.pt`` exist.\n",
        "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
        "            otherwise from ``test.pt``.\n",
        "        download (bool, optional): If true, downloads the dataset from the internet and\n",
        "            puts it in root directory. If dataset is already downloaded, it is not\n",
        "            downloaded again.\n",
        "        transform (callable, optional): A function/transform that takes in a numpy image\n",
        "            and may return a horizontally flipped image.\n",
        "    \"\"\"\n",
        "\n",
        "\turls = \t[\n",
        "\t\t\t\t'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n",
        "        \t\t'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz',\n",
        "        \t\t'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n",
        "       \t\t\t'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz'\n",
        "    \t   \t]\n",
        "\n",
        "\tfile_name =\t[\n",
        "    \t\t\t \t'train-images-idx3-ubyte',\n",
        "    \t\t\t \t'train-labels-idx1-ubyte',\n",
        "    \t\t\t \t't10k-images-idx3-ubyte',\n",
        "    \t\t\t \t't10k-labels-idx1-ubyte'\n",
        "    \t\t\t]\n",
        "\n",
        "\traw = \"raw\"\n",
        "\tprocesssed = \"processsed\"\n",
        "\n",
        "\n",
        "\tdef __init__(self, root, train=True, transform=True, download=False):\n",
        "\t\tsuper(Fashion, self).__init__()\n",
        "\t\tself.root = root\n",
        "\t\tself.transform = transform\n",
        "\t\tself.train = train\n",
        "\t\tself.tensor_transform = transforms.ToTensor()\n",
        "\n",
        "\t\traw_path = os.path.join(self.root,self.raw)\n",
        "\t\tif download and (os.path.exists(raw_path) == False):\n",
        "\t\t\tself.download(self.root)\n",
        "\n",
        "\t\tif self.train:\n",
        "\t\t\ttrain_path = os.path.join(self.root,self.processsed,\"training_set.pt\")\n",
        "\t\t\tself.train_images, self.train_labels = torch.load(train_path)\n",
        "\t\telse:\n",
        "\t\t\ttest_path = os.path.join(self.root,self.processsed,\"testing_set.pt\")\n",
        "\t\t\tself.test_images, self.test_labels = torch.load(test_path)\n",
        "\n",
        "\n",
        "\tdef __getitem__(self,index):\n",
        "\t\tif self.train:\n",
        "\t\t\timage, label = self.train_images[index], self.train_labels[index]\n",
        "\t\telse:\n",
        "\t\t\timage, label = self.test_images[index], self.test_labels[index]\n",
        "\n",
        "\t\timage = image.numpy()\n",
        "\t\timage = np.rot90(image,axes = (1,2)).copy()\n",
        "\n",
        "\t\tif self.transform and self.train:\n",
        "\t\t\timage = self.transform_process(image)\n",
        "\n",
        "\t\timage = self.tensor_transform(image)\n",
        "\t\timage = image.contiguous()\n",
        "\t\timage = image.view(1,28,28)\n",
        "\t\t\t\n",
        "\t\treturn image,label\n",
        "\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\tif self.train:\n",
        "\t\t\treturn(len(self.train_images))\n",
        "\t\telse:\n",
        "\t\t\treturn(len(self.test_images))\n",
        "\n",
        "\n",
        "\tdef transform_process(self, image):\n",
        "\t\tself.rotate = random.getrandbits(1)\n",
        "\t\timage = np.flip(image,self.rotate).copy()\n",
        "\t\treturn image\n",
        "\n",
        "\n",
        "\tdef download(self, root):\n",
        "\t\traw_path = os.path.join(self.root,self.raw)\n",
        "\t\tprocesssed_path = os.path.join(self.root,self.processsed)\n",
        "\n",
        "\t\ttry:\n",
        "\t\t\tos.makedirs(raw_path)\n",
        "\t\t\tos.makedirs(processsed_path)\n",
        "\t\texcept OSError as exc:\n",
        "\t\t\tif exc.errno != errno.EEXIST:\n",
        "\t\t\t\traise\n",
        "\t\t\tpass\n",
        "\n",
        "\t\tfor file_index in range(len(self.file_name)):\n",
        "\t\t\tprint(\"Downloading:\",self.urls[file_index])\n",
        "\t\t\turllib.request.urlretrieve(self.urls[file_index],(self.file_name[file_index]+'.gz'))\n",
        "\t\t\tprint(\"Extracting:\",self.file_name[file_index]+\".gz\")\n",
        "\t\t\tf = gzip.open(self.file_name[file_index]+'.gz', 'rb')\n",
        "\t\t\twith open(raw_path+\"/\"+self.file_name[file_index],'wb') as w:\n",
        "\t\t\t\tfor line in f.readlines():\n",
        "\t\t\t\t\tw.write(line)\t\n",
        "\t\t\tf.close()\n",
        "\t\t\tos.remove(self.file_name[file_index]+\".gz\")\n",
        "\n",
        "\t\tprint()\n",
        "\t\tprint(\"Raw data downloaded and extracted in your specified root directory under /raw\")\n",
        "\t\tprint()\n",
        "\t\tself.process(self.root)\n",
        "\n",
        "\n",
        "\tdef process(self, root):\n",
        "\t\traw_path = os.path.join(self.root,self.raw)\n",
        "\t\tprocesssed_path = os.path.join(self.root,self.processsed)\n",
        "\n",
        "\t\tprint(\"Processing training data\")\n",
        "\t\ttrain_image = self.readimg(self.root, self.file_name[0], 2051)\n",
        "\t\ttrain_label\t= self.readlab(self.root, self.file_name[1], 2049)\n",
        "\t\ttrain_data = (train_image,train_label)\n",
        "\n",
        "\t\tprint(\"Processing testing data\")\n",
        "\t\ttest_image = self.readimg(self.root, self.file_name[2], 2051)\n",
        "\t\ttest_label = self.readlab(self.root, self.file_name[3], 2049)\n",
        "\t\ttest_data = (test_image,test_label)\n",
        "\n",
        "\t\ttrain_path = os.path.join(self.root,self.processsed,\"training_set.pt\")\n",
        "\t\twith open(train_path,\"wb\") as f:\n",
        "\t\t\ttorch.save(train_data,f)\n",
        "\n",
        "\t\ttest_path = os.path.join(self.root,self.processsed,\"testing_set.pt\")\n",
        "\t\twith open(test_path,\"wb\") as f:\n",
        "\t\t\ttorch.save(test_data,f)\n",
        "\t\tprint()\n",
        "\t\tprint(\"Processed data has been stored in your specified root directory under /processsed\")\n",
        "\t\tprint()\n",
        "\n",
        "\n",
        "\tdef readimg(self, root, file, magic):\n",
        "\t\timage = []\n",
        "\t\tpath = os.path.join(self.root,self.raw,file)\n",
        "\t\twith open(path,'rb') as f:\n",
        "\t\t\tmagic_number, size, row, col = struct.unpack('>IIII',f.read(16))\n",
        "\t\t\tassert (magic_number == magic)\n",
        "\t\t\tfor run in range(size*row*col):\n",
        "\t\t\t\timage.append(list(struct.unpack('B',f.read(1)))[0])\n",
        "\t\t\timage = np.asarray(image, dtype = np.float32)\n",
        "\t\t\treturn (torch.from_numpy(image).view(size,1,row,col))\n",
        "\n",
        "\n",
        "\tdef readlab(self, root, file, magic):\n",
        "\t\tlabel = []\n",
        "\t\tpath = os.path.join(self.root,self.raw,file)\n",
        "\t\twith open(path,'rb') as f:\n",
        "\t\t\tmagic_number, size = struct.unpack(\">II\",f.read(8))\n",
        "\t\t\tassert (magic_number == magic)\n",
        "\t\t\tfor run in range(size):\n",
        "\t\t\t\tlabel.append(list(struct.unpack('b',f.read(1)))[0])\n",
        "\t\t\tlabel = np.asarray(label)\n",
        "\t\t\treturn (torch.from_numpy(label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "78LYxft05tKZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initialising Dataset Class"
      ]
    },
    {
      "metadata": {
        "id": "6y7BE7IP5tKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "142fe777-a25c-4a48-d0a5-0fb43950555f"
      },
      "cell_type": "code",
      "source": [
        "train_dataset = Fashion(root = \"./FashionMNIST\", train = True, transform = True, download = True)\n",
        "test_dataset = Fashion(root = \"./FashionMNIST\", train = False, transform = True, download = True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Extracting: train-images-idx3-ubyte.gz\n",
            "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Extracting: train-labels-idx1-ubyte.gz\n",
            "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Extracting: t10k-images-idx3-ubyte.gz\n",
            "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Extracting: t10k-labels-idx1-ubyte.gz\n",
            "\n",
            "Raw data downloaded and extracted in your specified root directory under /raw\n",
            "\n",
            "Processing training data\n",
            "Processing testing data\n",
            "\n",
            "Processed data has been stored in your specified root directory under /processsed\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i5pWBGNB5tKm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Defining Batch Size and Total Iterations"
      ]
    },
    {
      "metadata": {
        "id": "pbeidJ-x5tKo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "n_iters = 18000\n",
        "epoch_size = n_iters/(len(train_dataset)/batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "daUTWj3S5tKt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Making the Test Dataset Iterable"
      ]
    },
    {
      "metadata": {
        "id": "ZSeysw_85tKw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wU40eaaj5tK3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neurel Network Model"
      ]
    },
    {
      "metadata": {
        "id": "UFqFQipc5tK5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper (CNNModel, self).__init__()\n",
        "\n",
        "\t\tself.cnn1 = nn.Conv2d(in_channels = 1, out_channels = 32, kernel_size = 5, stride = 1, padding = 2)\n",
        "\t\tself.relu1 = nn.ReLU()\n",
        "\t\tself.norm1 = nn.BatchNorm2d(32)\n",
        "\t\tnn.init.xavier_uniform(self.cnn1.weight)\n",
        "\n",
        "\t\tself.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "\t\tself.cnn2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 1, padding = 2)\n",
        "\t\tself.relu2 = nn.ReLU()\n",
        "\t\tself.norm2 = nn.BatchNorm2d(64)\n",
        "\t\tnn.init.xavier_uniform(self.cnn2.weight)\n",
        "\n",
        "\t\tself.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "\t\tself.fc1 = nn.Linear(4096, 4096)\n",
        "\t\tself.fcrelu = nn.ReLU()\n",
        "\n",
        "\t\tself.fc2 = nn.Linear(4096, 10)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = self.cnn1(x)\n",
        "\t\tout = self.relu1(out)\n",
        "\t\tout = self.norm1(out)\n",
        "\n",
        "\t\tout = self.maxpool1(out)\n",
        "\n",
        "\t\tout = self.cnn2(out)\n",
        "\t\tout = self.relu2(out)\n",
        "\t\tout = self.norm2(out)\n",
        "\n",
        "\t\tout = self.maxpool2(out)\n",
        "\n",
        "\t\tout = out.view(out.size(0),-1)\n",
        "\n",
        "\t\tout = self.fc1(out)\n",
        "\t\tout = self.fcrelu(out)\n",
        "\n",
        "\t\tout = self.fc2(out)\n",
        "\t\treturn out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G4gocgl15tLA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Instantiate Model Class"
      ]
    },
    {
      "metadata": {
        "id": "ouuRSYNG5tLC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = CNNModel()\n",
        "if torch.cuda.is_available():\n",
        "\tmodel.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6IJ2ioiL5tLL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Instantiate Loss and Optimizer Class"
      ]
    },
    {
      "metadata": {
        "id": "P-U2_VqD5tLN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 0.015\n",
        "optimizer = optim.SGD(model.parameters(),lr = learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oLMso3jA5tLY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training the Model\n",
        "For each epoch a new training set will be loaded for increasing the randomness in horizontal flips of MNIST images."
      ]
    },
    {
      "metadata": {
        "id": "0e1fhDnd5tLY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "c2ccef77-dda7-4656-8ee5-c3b42b047539"
      },
      "cell_type": "code",
      "source": [
        "iter = 0\n",
        "for epoch in range(int(math.ceil(epoch_size))):\n",
        "\ttrain_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
        "\tfor i, (images, labels) in enumerate(train_loader):\n",
        "\t\tif torch.cuda.is_available():\n",
        "\t\t\timages = Variable(images.cuda())\n",
        "\t\t\tlabels = Variable(labels.cuda())\n",
        "\t\telse:\n",
        "\t\t\timages = Variable(images)\n",
        "\t\t\tlabels = Variable(labels)\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\toutputs = model(images)\n",
        "\t\tloss = criterion(outputs,labels)\n",
        "\t\tloss.backward()\n",
        "\t\toptimizer.step()\n",
        "\t\titer += 1\n",
        "\n",
        "\t\tif iter%3000 == 0:\n",
        "\t\t\tcorrect = 0\n",
        "\t\t\ttotal = 0\n",
        "\t\t\tfor image,label in test_loader:\n",
        "\t\t\t\tif torch.cuda.is_available():\n",
        "\t\t\t\t\timage = Variable(image.cuda())\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\timage = Variable(image)\n",
        "\t\t\t\toutput = model(image)\n",
        "\t\t\t\t_, predicted = torch.max(output.data,1)\n",
        "\t\t\t\ttotal += label.size(0)\n",
        "\t\t\t\tif torch.cuda.is_available():\n",
        "\t\t\t\t\tcorrect += (predicted.cpu() == label.cpu()).sum()\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tcorrect += (predicted == label).sum()\n",
        "\n",
        "\t\t\taccuracy = 100 * (correct/total)\n",
        "\t\t\tprint('Iteration: {} Loss: {} Accuracy: {}'.format(iter, loss.data[0], round(accuracy,2)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 3000 Loss: 0.23688818514347076 Accuracy: 90.01\n",
            "Iteration: 6000 Loss: 0.17669464647769928 Accuracy: 91.64\n",
            "Iteration: 9000 Loss: 0.11591944098472595 Accuracy: 91.7\n",
            "Iteration: 12000 Loss: 0.06624385714530945 Accuracy: 92.34\n",
            "Iteration: 15000 Loss: 0.14146874845027924 Accuracy: 92.47\n",
            "Iteration: 18000 Loss: 0.029156431555747986 Accuracy: 92.48\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
